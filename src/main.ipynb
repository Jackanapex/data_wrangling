{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries for webscraping\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "# Import requried utility libraries\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get urls from the input folder.\n",
    "It is assumed that the input folder is at the same level with src, and the file holding the urls is called from_urls. Also it is assumed that the urls are stored and separated by newlines in the input file.\\\n",
    "It is also assumed that only the first two lines of the input file will be valid URLs. No verificiation is carried out for the input data as it is outside the assignment scope\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the url list\n",
    "url_dict = {\n",
    "    'season_126': '',\n",
    "    'season_womens_seven': ''\n",
    "}\n",
    "\n",
    "# Define the import file path\n",
    "INPUT_FILE_NAME = 'from_urls'\n",
    "\n",
    "url_file_fn = os.path.join(\n",
    "    os.path.abspath('..'), \n",
    "    'input',\n",
    "    INPUT_FILE_NAME\n",
    ")\n",
    "\n",
    "# Open the file and read the urls into the url list\n",
    "url_file = open(url_file_fn, 'r')\n",
    "for k, v in url_dict.items():\n",
    "    line = url_file.readline()\n",
    "    if line:\n",
    "        url_dict[k] = line.strip()\n",
    "    else:\n",
    "        pass\n",
    "url_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request data from Wikipedia for AFL Season 126 and AFL Women's Season Seven\n",
    "The raw data will also be saved into a dictionary of two BeautifulSoup objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the BeautifulSoup object dictionary\n",
    "bs_dict = {\n",
    "    'season_126': None,\n",
    "    'season_womens_seven': None\n",
    "}\n",
    "\n",
    "# Iterate through url_dict and save the BeautifulSoup object to bs_dict\n",
    "for k, v in url_dict.items():\n",
    "    response = requests.get(v)\n",
    "    html = response.text\n",
    "    bs_dict[k] = BeautifulSoup(html, 'html.parser')\n",
    "    response.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract from the BeautifuleSoup projects for the tables under \"Home-and-away season\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO helper functoin to extract the required elements of the round tables\n",
    "def extract_round_and_match_elements(soup_obj):\n",
    "    # Note: round elements should be a match bs object list\n",
    "    result_round_elements = []\n",
    "    found_round_list = soup_obj.find_all('to-do')\n",
    "    for round in found_round_list:\n",
    "        round_number = round.find('to-do')\n",
    "        matches = round.find_all('to-do')\n",
    "        for match in matches:\n",
    "            game_datetime = match.find('to-do')\n",
    "            home = match.find('to-do')\n",
    "            result = match.find('to-do')\n",
    "            away = match.find('to-do')\n",
    "            location = match.find('to-do')\n",
    "            result_round_elements.append(\n",
    "                {\n",
    "                    'round_number': round_number.text,\n",
    "                    'datetime': game_datetime.text,\n",
    "                    'home': home.text,\n",
    "                    'result': result.text,\n",
    "                    'away': away.text,\n",
    "                    'location': location.text\n",
    "                }\n",
    "            )\n",
    "    return result_round_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO: Iterate through the BeautifulSoup object and extract all the ROUND tables under \n",
    "# body -> div class mw-page-container -> div class mw-content-container\n",
    "# -> div class vector-body ve-init-mw-desktopArticleTarget-targetContainer\n",
    "# -> div class mw-body-content mw-content-ltr\n",
    "# -> div class mw-parser-output -> ALL non-\"wikitable\" class tables\n",
    "\n",
    "# Initialize a match_dict to store the extracted match element lists\n",
    "round_dict = {\n",
    "    'season_126': [],\n",
    "    'season_womens_seven': []\n",
    "}\n",
    "for k, v in bs_dict.items():\n",
    "    round_dict[k] = extract_round_and_match_elements(v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the match information from the round_dict and save to a dictionary of matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO: Iterate through the round elements and save the match elements to a list of strings\n",
    "def extract_match_elements_to_strings(rounds):\n",
    "    match_string_list = []\n",
    "    re_pattern_dict = {\n",
    "        'datetime': 'some regex pattern',\n",
    "        'home': 'some regex pattern',\n",
    "        'result': 'some regex pattern',\n",
    "        'away': 'some regex pattern',\n",
    "        'location': 'some regex pattern'\n",
    "    }\n",
    "    matched_re_group_dict = {\n",
    "        'datetime': None,\n",
    "        'home': None,\n",
    "        'result': None,\n",
    "        'away': None,\n",
    "        'location': None\n",
    "    }\n",
    "    for match in rounds:\n",
    "        # Hint: use regex here to find the below values from a round element\n",
    "        for k, v in re_pattern_dict.items():\n",
    "            matched_re_group_dict[k] = re.match(v, match[k])\n",
    "        this_match_str = ','.join([\n",
    "            match['round_number'],\n",
    "            matched_re_group_dict['datetime'].group(0),\n",
    "            matched_re_group_dict['home'].group(0)\n",
    "            # to-do: add all values required\n",
    "        ])\n",
    "        match_string_list.append(this_match_str)\n",
    "    return match_string_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO: iterate through the round_dict and extract the following information for each match\n",
    "# RegEx is used here per example 3.5 Part 3\n",
    "\"\"\"\n",
    "Round Number (example: \"1\")\n",
    "Day of the game (example: \"Friday\")\n",
    "Date of the game (example: \"19-Aug\")\n",
    "Time of the game (example: \"7:50 pm\")\n",
    "First team name (example: \"Brisbane Lions\")\n",
    "First team score in points only (example: \"57\")\n",
    "First team win status (either \"def. by\" or \"def.\")\n",
    "Second team name (example: \"Melbourne\")\n",
    "Second team score in points only (example: \"115\")\n",
    "Game location (example: \"The Gabba\")\n",
    "Stadium Attendees (example: \"32172\")\n",
    "\"\"\"\n",
    "\n",
    "# Initialize a match_dict to store the extracted match element lists\n",
    "csv_title_row = ','.join([\n",
    "    'Round Number',\n",
    "    'Day of the game',\n",
    "    'Date of the game',\n",
    "    'Time of the game',\n",
    "    'First team name',\n",
    "    'First team score',\n",
    "    'First team win status',\n",
    "    'Second team name',\n",
    "    'Second team score',\n",
    "    'Game location',\n",
    "    'Stadium Attendees'\n",
    "])\n",
    "match_string_list = []\n",
    "for k, v in round_dict.items():\n",
    "    match_string_list.extend(extract_match_elements_to_strings(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the result to a csv output\n",
    "It is assumed that the output folder is at the same level as the src folder \\\n",
    "It is also assumed that a csv file output named scraped_match_table.csv file will exist and contain the result \\\n",
    "The file will be overwritten regardless of its existing contents and no verification of existence will be carried out\\\n",
    "since it is outside the assignment scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output file path\n",
    "OUTPUT_FILE_NAME = 'scraped_match_table.csv'\n",
    "scraped_match_table_fn = os.path.join(\n",
    "    os.path.abspath('..'), \n",
    "    'output',\n",
    "    OUTPUT_FILE_NAME\n",
    ")\n",
    "\n",
    "# Write the result_list to the output file\n",
    "result_file = open(scraped_match_table_fn, 'w')\n",
    "for result_str in match_string_list:\n",
    "    result_file.write(f\"{result_str}\\n\")\n",
    "result_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
